{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "657637ae",
   "metadata": {},
   "source": [
    "# This notebook shows a step-by-step on how the semantic network is built"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80a92e1",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc820f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486b70c",
   "metadata": {},
   "source": [
    "## 1. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768605f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/materials_data_final.csv\"   # ['source', 'functions', 'application']\n",
    "\n",
    "# sBERT model (small, fast, good quality)\n",
    "SBERT_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Similarity graph parameters for *within-category* edges\n",
    "SIM_THRESHOLD = 0.62     # only add intra-category edges if cosine sim >= this\n",
    "TOP_K = 10               # OR, keep up to top-K neighbors per node (after threshold)\n",
    "\n",
    "# Function splitting\n",
    "FUNCTION_SEPARATORS = [\";\", \"|\", \"\\n\", \",\"]  # adjust if needed\n",
    "LOWERCASE_KEYS = True    # normalize node keys to avoid duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b428e",
   "metadata": {},
   "source": [
    "## 2. Load and check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e197bc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>functions</th>\n",
       "      <th>application</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>denim</td>\n",
       "      <td>[recycle-yarn, weave-fabric, support-innovatio...</td>\n",
       "      <td>large work of art presented to the dutch royal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bacterial dye</td>\n",
       "      <td>[produce pigments, create sustainable alternat...</td>\n",
       "      <td>microbial colour library for dyeing textiles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>basalt</td>\n",
       "      <td>[reinforce-fabric, prevent-algal growth, exten...</td>\n",
       "      <td>reinforcement fabric for maritime applications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flax fibers</td>\n",
       "      <td>[provide structural support, reduce environmen...</td>\n",
       "      <td>interior wall panels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cotton</td>\n",
       "      <td>[recycle-textiles, create-carpets, reduce-wast...</td>\n",
       "      <td>high-quality recycled carpets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          source                                          functions  \\\n",
       "0          denim  [recycle-yarn, weave-fabric, support-innovatio...   \n",
       "1  bacterial dye  [produce pigments, create sustainable alternat...   \n",
       "2         basalt  [reinforce-fabric, prevent-algal growth, exten...   \n",
       "3    flax fibers  [provide structural support, reduce environmen...   \n",
       "4         cotton  [recycle-textiles, create-carpets, reduce-wast...   \n",
       "\n",
       "                                         application  \n",
       "0  large work of art presented to the dutch royal...  \n",
       "1       microbial colour library for dyeing textiles  \n",
       "2     reinforcement fabric for maritime applications  \n",
       "3                               interior wall panels  \n",
       "4                      high-quality recycled carpets  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Basic sanity checks\n",
    "expected_cols = {\"source\", \"functions\", \"application\"}\n",
    "missing = expected_cols - set(df.columns)\n",
    "assert not missing, f\"Missing required columns: {missing}\"\n",
    "\n",
    "# Clean helpers\n",
    "def norm_text(s):\n",
    "    if pd.isna(s): \n",
    "        return \"\"\n",
    "    s = str(s).strip()\n",
    "    return s.lower() if LOWERCASE_KEYS else s\n",
    "\n",
    "def split_functions(s):\n",
    "    if pd.isna(s) or not str(s).strip():\n",
    "        return []\n",
    "    text = str(s)\n",
    "    for sep in FUNCTION_SEPARATORS:\n",
    "        text = text.replace(sep, \";\")\n",
    "    parts = [p.strip() for p in text.split(\";\")]\n",
    "    parts = [p for p in parts if p]  # drop empties\n",
    "    return parts\n",
    "\n",
    "# Normalize/expand rows\n",
    "rows = []\n",
    "for _, r in df.iterrows():\n",
    "    src = norm_text(r[\"source\"])\n",
    "    app = norm_text(r[\"application\"])\n",
    "    funcs = [norm_text(f) for f in split_functions(r[\"functions\"])]\n",
    "    # If no functions, keep an empty list (we'll still add source<->application)\n",
    "    rows.append({\"source\": src, \"functions\": funcs, \"application\": app})\n",
    "\n",
    "df_norm = pd.DataFrame(rows)\n",
    "df_norm.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e44f8f0",
   "metadata": {},
   "source": [
    "## 3. Build node dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d569ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sources=1314, #functions=4144, #applications=1342\n"
     ]
    }
   ],
   "source": [
    "# We’ll keep canonical keys for graph nodes, and also store a display label.\n",
    "# (Here, since we normalized to lower-case, the key == label; if you want\n",
    "#  nicer labels, keep a separate mapping.)\n",
    "\n",
    "sources = set()\n",
    "functions = set()\n",
    "applications = set()\n",
    "\n",
    "for r in rows:\n",
    "    if r[\"source\"]: sources.add(r[\"source\"])\n",
    "    if r[\"application\"]: applications.add(r[\"application\"])\n",
    "    for f in r[\"functions\"]:\n",
    "        functions.add(f)\n",
    "\n",
    "print(f\"#sources={len(sources)}, #functions={len(functions)}, #applications={len(applications)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c799c2",
   "metadata": {},
   "source": [
    "## 4. Initialize graph and add nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c7116b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph now has 6800 nodes.\n"
     ]
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "def add_nodes(category, items):\n",
    "    for x in items:\n",
    "        if not x: \n",
    "            continue\n",
    "        G.add_node(\n",
    "            x,\n",
    "            label=x,           # you can store original (non-lowercased) label if you kept it\n",
    "            category=category  # 'source' | 'function' | 'application'\n",
    "        )\n",
    "\n",
    "add_nodes(\"source\", sources)\n",
    "add_nodes(\"function\", functions)\n",
    "add_nodes(\"application\", applications)\n",
    "\n",
    "print(f\"Graph now has {G.number_of_nodes()} nodes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e820a3",
   "metadata": {},
   "source": [
    "## 5. Add inter-category edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdddd24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After co-occurrence edges: 10764 edges.\n"
     ]
    }
   ],
   "source": [
    "def bump_edge(u, v, edge_type=\"cooccurrence\", weight=1.0):\n",
    "    if u == v:\n",
    "        return\n",
    "    if G.has_edge(u, v):\n",
    "        # accumulate weight if already present\n",
    "        G[u][v][\"weight\"] += weight\n",
    "        # if we’re mixing edge types, we can keep a set; here we keep the last\n",
    "        G[u][v][\"edge_type\"] = edge_type\n",
    "    else:\n",
    "        G.add_edge(u, v, weight=weight, edge_type=edge_type)\n",
    "\n",
    "for r in rows:\n",
    "    s = r[\"source\"]\n",
    "    a = r[\"application\"]\n",
    "    f_list = r[\"functions\"]\n",
    "\n",
    "    # source <-> each function\n",
    "    for f in f_list:\n",
    "        if s and f:\n",
    "            bump_edge(s, f, edge_type=\"cooccurrence\", weight=1.0)\n",
    "\n",
    "    # source <-> application\n",
    "    #if s and a:\n",
    "    #    bump_edge(s, a, edge_type=\"cooccurrence\", weight=1.0)\n",
    "\n",
    "    # each function <-> application\n",
    "    for f in f_list:\n",
    "        if f and a:\n",
    "            bump_edge(f, a, edge_type=\"cooccurrence\", weight=1.0)\n",
    "\n",
    "print(f\"After co-occurrence edges: {G.number_of_edges()} edges.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab2164a",
   "metadata": {},
   "source": [
    "## 6. Embed each category with sBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce3ff799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bba984cb02b4933b3ee09087e7d8821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42f1af57c1545b69a4d21c9ef2d61ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb688c1f5bb42df84b19b752de49850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((1314, 384), (4144, 384), (1342, 384))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer(SBERT_MODEL)\n",
    "\n",
    "def embed_list(items):\n",
    "    # items: list of node keys (strings)\n",
    "    # returns np.array (n, d)\n",
    "    if not items:\n",
    "        return np.empty((0, 384))  # MiniLM dim\n",
    "    return np.array(model.encode(items, show_progress_bar=True, normalize_embeddings=True))\n",
    "\n",
    "# Prepare per-category lists for stable ordering\n",
    "src_list = sorted(list(sources))\n",
    "fun_list = sorted(list(functions))\n",
    "app_list = sorted(list(applications))\n",
    "\n",
    "src_emb = embed_list(src_list)\n",
    "fun_emb = embed_list(fun_list)\n",
    "app_emb = embed_list(app_list)\n",
    "\n",
    "src_emb.shape, fun_emb.shape, app_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6e456",
   "metadata": {},
   "source": [
    "## 7. Add intra-category semantic edges with nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cb7de85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic edges added — sources: 4971, functions: 15845, applications: 5115\n",
      "Total edges now: 36695\n"
     ]
    }
   ],
   "source": [
    "def add_semantic_edges(labels, embeddings, edge_type=\"semantic\",\n",
    "                       sim_threshold=SIM_THRESHOLD, top_k=TOP_K):\n",
    "    n = len(labels)\n",
    "    if n <= 1:\n",
    "        return 0\n",
    "\n",
    "    # NearestNeighbors with cosine metric returns distances; similarity = 1 - distance\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(top_k+1, n), metric=\"cosine\", algorithm=\"auto\")\n",
    "    nbrs.fit(embeddings)\n",
    "    distances, indices = nbrs.kneighbors(embeddings)\n",
    "\n",
    "    added = 0\n",
    "    for i, (dists, nbr_idx) in enumerate(zip(distances, indices)):\n",
    "        u = labels[i]\n",
    "        for dist, j in zip(dists, nbr_idx):\n",
    "            if i == j:\n",
    "                continue\n",
    "            sim = 1.0 - float(dist)\n",
    "            if sim >= sim_threshold:\n",
    "                v = labels[j]\n",
    "                # Avoid inter-category mixing here — we're passed a single category list\n",
    "                if not G.has_edge(u, v):\n",
    "                    G.add_edge(u, v, weight=sim, edge_type=edge_type)\n",
    "                    added += 1\n",
    "                else:\n",
    "                    # If an edge already exists (e.g., from cooccurrence), keep the larger weight?\n",
    "                    # Here we keep both concepts: cooccurrence edges are counts ≥1; semantic are ≤1\n",
    "                    # We’ll simply set the edge_type to 'semantic+cooccurrence' if both existed.\n",
    "                    e = G[u][v]\n",
    "                    if e.get(\"edge_type\") == \"cooccurrence\":\n",
    "                        e[\"edge_type\"] = \"semantic+cooccurrence\"\n",
    "                        # Keep the cooccurrence weight (count) as-is.\n",
    "                        # Optionally store a separate field:\n",
    "                        e[\"semantic_sim\"] = sim\n",
    "                    else:\n",
    "                        # Another semantic candidate; update to max sim\n",
    "                        e[\"weight\"] = max(e[\"weight\"], sim)\n",
    "                        e[\"edge_type\"] = \"semantic\"\n",
    "    return added\n",
    "\n",
    "added_src = add_semantic_edges(src_list, src_emb, edge_type=\"semantic\")\n",
    "added_fun = add_semantic_edges(fun_list, fun_emb, edge_type=\"semantic\")\n",
    "added_app = add_semantic_edges(app_list, app_emb, edge_type=\"semantic\")\n",
    "\n",
    "print(f\"Semantic edges added — sources: {added_src}, functions: {added_fun}, applications: {added_app}\")\n",
    "print(f\"Total edges now: {G.number_of_edges()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b681e3fa",
   "metadata": {},
   "source": [
    "## 8. Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78531412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node categories: Counter({'function': 4144, 'application': 1342, 'source': 1314})\n",
      "Edge types: Counter({'semantic': 25931, 'cooccurrence': 10764})\n",
      "\n",
      "Top neighbors for: optical tiles\n",
      "  - reflect-light  (w=1.000, type=cooccurrence)\n",
      "  - sculpt-image  (w=1.000, type=cooccurrence)\n",
      "  - transform-surface  (w=1.000, type=cooccurrence)\n",
      "  - ceramic tiles  (w=0.667, type=semantic)\n"
     ]
    }
   ],
   "source": [
    "# Node counts by category\n",
    "from collections import Counter\n",
    "cats = Counter(nx.get_node_attributes(G, \"category\").values())\n",
    "print(\"Node categories:\", cats)\n",
    "\n",
    "# Edge-type distribution\n",
    "etype_counts = Counter(nx.get_edge_attributes(G, \"edge_type\").values())\n",
    "print(\"Edge types:\", etype_counts)\n",
    "\n",
    "# Example: top neighbors by weight for a sample node\n",
    "def top_neighbors(node, k=10):\n",
    "    if node not in G:\n",
    "        return []\n",
    "    nbrs = []\n",
    "    for v in G[node]:\n",
    "        nbrs.append((v, G[node][v].get(\"weight\", 0.0), G[node][v].get(\"edge_type\", \"\")))\n",
    "    return sorted(nbrs, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "sample = next(iter(sources)) if sources else None\n",
    "if sample:\n",
    "    print(f\"\\nTop neighbors for: {sample}\")\n",
    "    for v, w, et in top_neighbors(sample, k=10):\n",
    "        print(f\"  - {v}  (w={w:.3f}, type={et})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d04e2f9",
   "metadata": {},
   "source": [
    "## 9. Save graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62de33c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: GEXF, GraphML, nodes.csv, edges.csv\n"
     ]
    }
   ],
   "source": [
    "# Graph files\n",
    "nx.write_gexf(G, \"materials_semantic_network.gexf\")     # Gephi/pyvis friendly\n",
    "nx.write_graphml(G, \"materials_semantic_network.graphml\")\n",
    "\n",
    "# Optional: CSV edge list\n",
    "edge_rows = []\n",
    "for u, v, data in G.edges(data=True):\n",
    "    edge_rows.append({\n",
    "        \"u\": u,\n",
    "        \"v\": v,\n",
    "        \"weight\": data.get(\"weight\", 1.0),\n",
    "        \"edge_type\": data.get(\"edge_type\", \"\")\n",
    "    })\n",
    "pd.DataFrame(edge_rows).to_csv(\"materials_semantic_edges.csv\", index=False)\n",
    "\n",
    "# Optional: Save node table\n",
    "node_rows = []\n",
    "for n, data in G.nodes(data=True):\n",
    "    node_rows.append({\n",
    "        \"node\": n,\n",
    "        \"label\": data.get(\"label\", n),\n",
    "        \"category\": data.get(\"category\", \"\")\n",
    "    })\n",
    "pd.DataFrame(node_rows).to_csv(\"materials_semantic_nodes.csv\", index=False)\n",
    "\n",
    "print(\"Saved: GEXF, GraphML, nodes.csv, edges.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c1634c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
